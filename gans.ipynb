{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f71332c3",
   "metadata": {},
   "source": [
    "# Assignment 4 - Generative Adversarial Networks\n",
    " \n",
    "**Authors:**\n",
    "\n",
    "1.   Liav Bachar 205888472\n",
    "2.   Naor Kolet 205533060\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee5a147",
   "metadata": {
    "id": "I2z5iHmg0oke"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8820f4e7",
   "metadata": {
    "id": "pT--0V_r0Q4B"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import log_loss\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder, OrdinalEncoder\n",
    " \n",
    "# Plots\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Misc.\n",
    "from scipy.io import arff\n",
    "import os\n",
    "import random\n",
    "import joblib\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3333ae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "def set_seed():    \n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76483324",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da6b5bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_arff_dataset(arff_path):\n",
    "    data = arff.loadarff(arff_path)\n",
    "    df = pd.DataFrame(data=data[0], columns=data[1].names())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dded780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/uciml/pima-indians-diabetes-database\n",
    "diab_df = load_arff_dataset(r'datasets/diabetes.arff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a997276c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>insu</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50.0</td>\n",
       "      <td>b'tested_positive'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31.0</td>\n",
       "      <td>b'tested_negative'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32.0</td>\n",
       "      <td>b'tested_positive'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21.0</td>\n",
       "      <td>b'tested_negative'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33.0</td>\n",
       "      <td>b'tested_positive'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63.0</td>\n",
       "      <td>b'tested_negative'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27.0</td>\n",
       "      <td>b'tested_negative'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30.0</td>\n",
       "      <td>b'tested_negative'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47.0</td>\n",
       "      <td>b'tested_positive'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23.0</td>\n",
       "      <td>b'tested_negative'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     preg   plas  pres  skin   insu  mass   pedi   age               class\n",
       "0     6.0  148.0  72.0  35.0    0.0  33.6  0.627  50.0  b'tested_positive'\n",
       "1     1.0   85.0  66.0  29.0    0.0  26.6  0.351  31.0  b'tested_negative'\n",
       "2     8.0  183.0  64.0   0.0    0.0  23.3  0.672  32.0  b'tested_positive'\n",
       "3     1.0   89.0  66.0  23.0   94.0  28.1  0.167  21.0  b'tested_negative'\n",
       "4     0.0  137.0  40.0  35.0  168.0  43.1  2.288  33.0  b'tested_positive'\n",
       "..    ...    ...   ...   ...    ...   ...    ...   ...                 ...\n",
       "763  10.0  101.0  76.0  48.0  180.0  32.9  0.171  63.0  b'tested_negative'\n",
       "764   2.0  122.0  70.0  27.0    0.0  36.8  0.340  27.0  b'tested_negative'\n",
       "765   5.0  121.0  72.0  23.0  112.0  26.2  0.245  30.0  b'tested_negative'\n",
       "766   1.0  126.0  60.0   0.0    0.0  30.1  0.349  47.0  b'tested_positive'\n",
       "767   1.0   93.0  70.0  31.0    0.0  30.4  0.315  23.0  b'tested_negative'\n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3d97bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)\n",
    "cred_df = load_arff_dataset(r'datasets/german_credit.arff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02fd5958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'A11'</td>\n",
       "      <td>6.0</td>\n",
       "      <td>b'A34'</td>\n",
       "      <td>b'A43'</td>\n",
       "      <td>1169.0</td>\n",
       "      <td>b'A65'</td>\n",
       "      <td>b'A75'</td>\n",
       "      <td>4.0</td>\n",
       "      <td>b'A93'</td>\n",
       "      <td>b'A101'</td>\n",
       "      <td>...</td>\n",
       "      <td>b'A121'</td>\n",
       "      <td>67.0</td>\n",
       "      <td>b'A143'</td>\n",
       "      <td>b'A152'</td>\n",
       "      <td>2.0</td>\n",
       "      <td>b'A173'</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'A192'</td>\n",
       "      <td>b'A201'</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'A12'</td>\n",
       "      <td>48.0</td>\n",
       "      <td>b'A32'</td>\n",
       "      <td>b'A43'</td>\n",
       "      <td>5951.0</td>\n",
       "      <td>b'A61'</td>\n",
       "      <td>b'A73'</td>\n",
       "      <td>2.0</td>\n",
       "      <td>b'A92'</td>\n",
       "      <td>b'A101'</td>\n",
       "      <td>...</td>\n",
       "      <td>b'A121'</td>\n",
       "      <td>22.0</td>\n",
       "      <td>b'A143'</td>\n",
       "      <td>b'A152'</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'A173'</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'A191'</td>\n",
       "      <td>b'A201'</td>\n",
       "      <td>b'2'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'A14'</td>\n",
       "      <td>12.0</td>\n",
       "      <td>b'A34'</td>\n",
       "      <td>b'A46'</td>\n",
       "      <td>2096.0</td>\n",
       "      <td>b'A61'</td>\n",
       "      <td>b'A74'</td>\n",
       "      <td>2.0</td>\n",
       "      <td>b'A93'</td>\n",
       "      <td>b'A101'</td>\n",
       "      <td>...</td>\n",
       "      <td>b'A121'</td>\n",
       "      <td>49.0</td>\n",
       "      <td>b'A143'</td>\n",
       "      <td>b'A152'</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'A172'</td>\n",
       "      <td>2.0</td>\n",
       "      <td>b'A191'</td>\n",
       "      <td>b'A201'</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'A11'</td>\n",
       "      <td>42.0</td>\n",
       "      <td>b'A32'</td>\n",
       "      <td>b'A42'</td>\n",
       "      <td>7882.0</td>\n",
       "      <td>b'A61'</td>\n",
       "      <td>b'A74'</td>\n",
       "      <td>2.0</td>\n",
       "      <td>b'A93'</td>\n",
       "      <td>b'A103'</td>\n",
       "      <td>...</td>\n",
       "      <td>b'A122'</td>\n",
       "      <td>45.0</td>\n",
       "      <td>b'A143'</td>\n",
       "      <td>b'A153'</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'A173'</td>\n",
       "      <td>2.0</td>\n",
       "      <td>b'A191'</td>\n",
       "      <td>b'A201'</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'A11'</td>\n",
       "      <td>24.0</td>\n",
       "      <td>b'A33'</td>\n",
       "      <td>b'A40'</td>\n",
       "      <td>4870.0</td>\n",
       "      <td>b'A61'</td>\n",
       "      <td>b'A73'</td>\n",
       "      <td>3.0</td>\n",
       "      <td>b'A93'</td>\n",
       "      <td>b'A101'</td>\n",
       "      <td>...</td>\n",
       "      <td>b'A124'</td>\n",
       "      <td>53.0</td>\n",
       "      <td>b'A143'</td>\n",
       "      <td>b'A153'</td>\n",
       "      <td>2.0</td>\n",
       "      <td>b'A173'</td>\n",
       "      <td>2.0</td>\n",
       "      <td>b'A191'</td>\n",
       "      <td>b'A201'</td>\n",
       "      <td>b'2'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>b'A14'</td>\n",
       "      <td>12.0</td>\n",
       "      <td>b'A32'</td>\n",
       "      <td>b'A42'</td>\n",
       "      <td>1736.0</td>\n",
       "      <td>b'A61'</td>\n",
       "      <td>b'A74'</td>\n",
       "      <td>3.0</td>\n",
       "      <td>b'A92'</td>\n",
       "      <td>b'A101'</td>\n",
       "      <td>...</td>\n",
       "      <td>b'A121'</td>\n",
       "      <td>31.0</td>\n",
       "      <td>b'A143'</td>\n",
       "      <td>b'A152'</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'A172'</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'A191'</td>\n",
       "      <td>b'A201'</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>b'A11'</td>\n",
       "      <td>30.0</td>\n",
       "      <td>b'A32'</td>\n",
       "      <td>b'A41'</td>\n",
       "      <td>3857.0</td>\n",
       "      <td>b'A61'</td>\n",
       "      <td>b'A73'</td>\n",
       "      <td>4.0</td>\n",
       "      <td>b'A91'</td>\n",
       "      <td>b'A101'</td>\n",
       "      <td>...</td>\n",
       "      <td>b'A122'</td>\n",
       "      <td>40.0</td>\n",
       "      <td>b'A143'</td>\n",
       "      <td>b'A152'</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'A174'</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'A192'</td>\n",
       "      <td>b'A201'</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>b'A14'</td>\n",
       "      <td>12.0</td>\n",
       "      <td>b'A32'</td>\n",
       "      <td>b'A43'</td>\n",
       "      <td>804.0</td>\n",
       "      <td>b'A61'</td>\n",
       "      <td>b'A75'</td>\n",
       "      <td>4.0</td>\n",
       "      <td>b'A93'</td>\n",
       "      <td>b'A101'</td>\n",
       "      <td>...</td>\n",
       "      <td>b'A123'</td>\n",
       "      <td>38.0</td>\n",
       "      <td>b'A143'</td>\n",
       "      <td>b'A152'</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'A173'</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'A191'</td>\n",
       "      <td>b'A201'</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>b'A11'</td>\n",
       "      <td>45.0</td>\n",
       "      <td>b'A32'</td>\n",
       "      <td>b'A43'</td>\n",
       "      <td>1845.0</td>\n",
       "      <td>b'A61'</td>\n",
       "      <td>b'A73'</td>\n",
       "      <td>4.0</td>\n",
       "      <td>b'A93'</td>\n",
       "      <td>b'A101'</td>\n",
       "      <td>...</td>\n",
       "      <td>b'A124'</td>\n",
       "      <td>23.0</td>\n",
       "      <td>b'A143'</td>\n",
       "      <td>b'A153'</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'A173'</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'A192'</td>\n",
       "      <td>b'A201'</td>\n",
       "      <td>b'2'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>b'A12'</td>\n",
       "      <td>45.0</td>\n",
       "      <td>b'A34'</td>\n",
       "      <td>b'A41'</td>\n",
       "      <td>4576.0</td>\n",
       "      <td>b'A62'</td>\n",
       "      <td>b'A71'</td>\n",
       "      <td>3.0</td>\n",
       "      <td>b'A93'</td>\n",
       "      <td>b'A101'</td>\n",
       "      <td>...</td>\n",
       "      <td>b'A123'</td>\n",
       "      <td>27.0</td>\n",
       "      <td>b'A143'</td>\n",
       "      <td>b'A152'</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'A173'</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'A191'</td>\n",
       "      <td>b'A201'</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1     2       3       4       5       6       7    8       9  \\\n",
       "0    b'A11'   6.0  b'A34'  b'A43'  1169.0  b'A65'  b'A75'  4.0  b'A93'   \n",
       "1    b'A12'  48.0  b'A32'  b'A43'  5951.0  b'A61'  b'A73'  2.0  b'A92'   \n",
       "2    b'A14'  12.0  b'A34'  b'A46'  2096.0  b'A61'  b'A74'  2.0  b'A93'   \n",
       "3    b'A11'  42.0  b'A32'  b'A42'  7882.0  b'A61'  b'A74'  2.0  b'A93'   \n",
       "4    b'A11'  24.0  b'A33'  b'A40'  4870.0  b'A61'  b'A73'  3.0  b'A93'   \n",
       "..      ...   ...     ...     ...     ...     ...     ...  ...     ...   \n",
       "995  b'A14'  12.0  b'A32'  b'A42'  1736.0  b'A61'  b'A74'  3.0  b'A92'   \n",
       "996  b'A11'  30.0  b'A32'  b'A41'  3857.0  b'A61'  b'A73'  4.0  b'A91'   \n",
       "997  b'A14'  12.0  b'A32'  b'A43'   804.0  b'A61'  b'A75'  4.0  b'A93'   \n",
       "998  b'A11'  45.0  b'A32'  b'A43'  1845.0  b'A61'  b'A73'  4.0  b'A93'   \n",
       "999  b'A12'  45.0  b'A34'  b'A41'  4576.0  b'A62'  b'A71'  3.0  b'A93'   \n",
       "\n",
       "          10  ...       12    13       14       15   16       17   18  \\\n",
       "0    b'A101'  ...  b'A121'  67.0  b'A143'  b'A152'  2.0  b'A173'  1.0   \n",
       "1    b'A101'  ...  b'A121'  22.0  b'A143'  b'A152'  1.0  b'A173'  1.0   \n",
       "2    b'A101'  ...  b'A121'  49.0  b'A143'  b'A152'  1.0  b'A172'  2.0   \n",
       "3    b'A103'  ...  b'A122'  45.0  b'A143'  b'A153'  1.0  b'A173'  2.0   \n",
       "4    b'A101'  ...  b'A124'  53.0  b'A143'  b'A153'  2.0  b'A173'  2.0   \n",
       "..       ...  ...      ...   ...      ...      ...  ...      ...  ...   \n",
       "995  b'A101'  ...  b'A121'  31.0  b'A143'  b'A152'  1.0  b'A172'  1.0   \n",
       "996  b'A101'  ...  b'A122'  40.0  b'A143'  b'A152'  1.0  b'A174'  1.0   \n",
       "997  b'A101'  ...  b'A123'  38.0  b'A143'  b'A152'  1.0  b'A173'  1.0   \n",
       "998  b'A101'  ...  b'A124'  23.0  b'A143'  b'A153'  1.0  b'A173'  1.0   \n",
       "999  b'A101'  ...  b'A123'  27.0  b'A143'  b'A152'  1.0  b'A173'  1.0   \n",
       "\n",
       "          19       20    21  \n",
       "0    b'A192'  b'A201'  b'1'  \n",
       "1    b'A191'  b'A201'  b'2'  \n",
       "2    b'A191'  b'A201'  b'1'  \n",
       "3    b'A191'  b'A201'  b'1'  \n",
       "4    b'A191'  b'A201'  b'2'  \n",
       "..       ...      ...   ...  \n",
       "995  b'A191'  b'A201'  b'1'  \n",
       "996  b'A192'  b'A201'  b'1'  \n",
       "997  b'A191'  b'A201'  b'1'  \n",
       "998  b'A192'  b'A201'  b'2'  \n",
       "999  b'A191'  b'A201'  b'1'  \n",
       "\n",
       "[1000 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5a4276",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da509ea",
   "metadata": {},
   "source": [
    "@attribute preg real\n",
    "@attribute plas real\n",
    "@attribute pres real\n",
    "@attribute skin real\n",
    "@attribute insu real\n",
    "@attribute mass real\n",
    "@attribute pedi real\n",
    "@attribute age real\n",
    "@attribute class {tested_negative,tested_positive}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "410b29f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "diab_ct = make_column_transformer(\n",
    "    (MinMaxScaler(), ['preg', 'plas', 'pres', 'skin', 'insu', 'mass', 'pedi', 'age']),\n",
    "    (OrdinalEncoder(), ['class']),\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bc1f32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((768, 8), (768,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diab_data = diab_ct.fit_transform(diab_df)\n",
    "diab_x, diab_y = diab_data[:, :-1], diab_data[:, -1]\n",
    "diab_x.shape, diab_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b54fc4c",
   "metadata": {},
   "source": [
    "@attribute 1 {A11, A12, A13, A14}\n",
    "@attribute 2 numeric \n",
    "@attribute 3 {A30, A31, A32, A33, A34}\n",
    "@attribute 4 {A40, A41, A42, A43, A44, A45, A46, A47, A48, A49, A410}\n",
    "@attribute 5 numeric\n",
    "@attribute 6 {A61, A62, A63, A64, A65}\n",
    "@attribute 7 {A71, A72, A73, A74, A75}\n",
    "@attribute 8 numeric\n",
    "@attribute 9 {A91, A92, A93, A94, A95}\n",
    "@attribute 10 {A101, A102, A103}\n",
    "@attribute 11 numeric\n",
    "@attribute 12 {A121, A122, A123, A124}\n",
    "@attribute 13 numeric\n",
    "@attribute 14 {A141, A142, A143}\n",
    "@attribute 15 {A151, A152, A153}\n",
    "@attribute 16 numeric\n",
    "@attribute 17 {A171, A172, A173, A174}\n",
    "@attribute 18 numeric\n",
    "@attribute 19 {A191, A192}\n",
    "@attribute 20 {A201, A202}\n",
    "@attribute 21 {1,2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35690b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "cred_ct = make_column_transformer(\n",
    "    (MinMaxScaler(), ['2', '5', '8', '11', '13', '16', '18']),\n",
    "    (OneHotEncoder(), ['1', '3', '4', '6', '7', '9', '10', '12', '14', '15', '17', '19', '20']),\n",
    "    (OrdinalEncoder(), ['21']),\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8966c1cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 61), (1000,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cred_data = cred_ct.fit_transform(cred_df)\n",
    "cred_x, cred_y = cred_data[:, :-1], cred_data[:, -1]\n",
    "cred_x.shape, cred_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e890dd1",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b8876d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model(noise_shape, output_shape):\n",
    "    inp = Input(shape=noise_shape)\n",
    "    \n",
    "    X = Dense(32, activation='relu')(inp)\n",
    "    X = Dense(64, activation='relu')(X)\n",
    "    X = Dense(128, activation='relu')(X)\n",
    "    \n",
    "    out = Dense(output_shape, activation='sigmoid')(X)\n",
    "    \n",
    "    model = Model(inp, out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad032638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_model(input_shape):\n",
    "    inp = Input(shape=input_shape)\n",
    "    \n",
    "    X = Dense(64, activation='relu')(inp)\n",
    "    X = Dense(32, activation='relu')(X)\n",
    "    X = Dense(16, activation='relu')(X)\n",
    "    \n",
    "    out = Dense(1, activation='sigmoid')(X)\n",
    "    \n",
    "    model = Model(inp, out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cef33ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real, fake):\n",
    "    real_loss = bce(tf.ones_like(real), real)\n",
    "    fake_loss = bce(tf.zeros_like(fake), fake)\n",
    "    total_loss = (real_loss + fake_loss) / 2\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake):\n",
    "    return bce(tf.ones_like(fake), fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b0b01788",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_opt = Adam(1e-4)\n",
    "genr_opt = Adam(1e-4)\n",
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "# @tf.function\n",
    "def train_step(samples, generator, discriminator, batch_size):\n",
    "    noise = tf.random.normal([batch_size, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_samples = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(samples, training=True)\n",
    "        fake_output = discriminator(generated_samples, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    genr_opt.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    disc_opt.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8eb43bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(train_data, epochs=50, noise_dim=5, batch_size=64, buffer_size=10000):\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_data).shuffle(buffer_size).batch(batch_size)\n",
    "    \n",
    "    discriminator = discriminator_model(train_data.shape[1])\n",
    "    generator = generator_model(noise_dim, train_data.shape[1])\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        gen_loss = 0\n",
    "        disc_loss = 0\n",
    "        num_batchs = 0\n",
    "        \n",
    "        for sample_batch in train_dataset:\n",
    "            num_batchs += 1\n",
    "            curr_gen_loss, curr_disc_loss = train_step(sample_batch, generator, discriminator, batch_size=batch_size)\n",
    "            gen_loss += curr_gen_loss\n",
    "            disc_loss += curr_disc_loss\n",
    "            \n",
    "        print (f'Time for epoch {epoch + 1} is {time.time()-start :.4f} sec')\n",
    "        print(f'\\tGenerator loss: {gen_loss/num_batchs:.4f}  Discriminator loss: {disc_loss/num_batchs:.4f}')\n",
    "        \n",
    "    return generator, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8a362670",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 1 is 0.2760 sec\n",
      "\tGenerator loss: 0.7318  Discriminator loss: 0.6867\n",
      "Time for epoch 2 is 0.2449 sec\n",
      "\tGenerator loss: 0.7498  Discriminator loss: 0.6805\n",
      "Time for epoch 3 is 0.2422 sec\n",
      "\tGenerator loss: 0.7639  Discriminator loss: 0.6757\n",
      "Time for epoch 4 is 0.2465 sec\n",
      "\tGenerator loss: 0.7743  Discriminator loss: 0.6720\n",
      "Time for epoch 5 is 0.2524 sec\n",
      "\tGenerator loss: 0.7811  Discriminator loss: 0.6692\n",
      "Time for epoch 6 is 0.2482 sec\n",
      "\tGenerator loss: 0.7858  Discriminator loss: 0.6666\n",
      "Time for epoch 7 is 0.2461 sec\n",
      "\tGenerator loss: 0.7883  Discriminator loss: 0.6647\n",
      "Time for epoch 8 is 0.2646 sec\n",
      "\tGenerator loss: 0.7914  Discriminator loss: 0.6625\n",
      "Time for epoch 9 is 0.2491 sec\n",
      "\tGenerator loss: 0.7891  Discriminator loss: 0.6623\n",
      "Time for epoch 10 is 0.2605 sec\n",
      "\tGenerator loss: 0.7888  Discriminator loss: 0.6615\n",
      "Time for epoch 11 is 0.2585 sec\n",
      "\tGenerator loss: 0.7879  Discriminator loss: 0.6615\n",
      "Time for epoch 12 is 0.2684 sec\n",
      "\tGenerator loss: 0.7860  Discriminator loss: 0.6618\n",
      "Time for epoch 13 is 0.2402 sec\n",
      "\tGenerator loss: 0.7845  Discriminator loss: 0.6624\n",
      "Time for epoch 14 is 0.2456 sec\n",
      "\tGenerator loss: 0.7823  Discriminator loss: 0.6634\n",
      "Time for epoch 15 is 0.2414 sec\n",
      "\tGenerator loss: 0.7801  Discriminator loss: 0.6641\n",
      "Time for epoch 16 is 0.2539 sec\n",
      "\tGenerator loss: 0.7780  Discriminator loss: 0.6648\n",
      "Time for epoch 17 is 0.2639 sec\n",
      "\tGenerator loss: 0.7803  Discriminator loss: 0.6635\n",
      "Time for epoch 18 is 0.2489 sec\n",
      "\tGenerator loss: 0.7794  Discriminator loss: 0.6636\n",
      "Time for epoch 19 is 0.2510 sec\n",
      "\tGenerator loss: 0.7754  Discriminator loss: 0.6649\n",
      "Time for epoch 20 is 0.2486 sec\n",
      "\tGenerator loss: 0.7645  Discriminator loss: 0.6700\n",
      "Time for epoch 21 is 0.2410 sec\n",
      "\tGenerator loss: 0.7471  Discriminator loss: 0.6793\n",
      "Time for epoch 22 is 0.2457 sec\n",
      "\tGenerator loss: 0.7229  Discriminator loss: 0.6923\n",
      "Time for epoch 23 is 0.2659 sec\n",
      "\tGenerator loss: 0.7008  Discriminator loss: 0.7017\n",
      "Time for epoch 24 is 0.2572 sec\n",
      "\tGenerator loss: 0.6820  Discriminator loss: 0.7050\n",
      "Time for epoch 25 is 0.2479 sec\n",
      "\tGenerator loss: 0.6851  Discriminator loss: 0.6978\n",
      "Time for epoch 26 is 0.2550 sec\n",
      "\tGenerator loss: 0.6866  Discriminator loss: 0.6872\n",
      "Time for epoch 27 is 0.2402 sec\n",
      "\tGenerator loss: 0.6927  Discriminator loss: 0.6769\n",
      "Time for epoch 28 is 0.2396 sec\n",
      "\tGenerator loss: 0.6951  Discriminator loss: 0.6712\n",
      "Time for epoch 29 is 0.2448 sec\n",
      "\tGenerator loss: 0.6913  Discriminator loss: 0.6704\n",
      "Time for epoch 30 is 0.2580 sec\n",
      "\tGenerator loss: 0.6810  Discriminator loss: 0.6747\n",
      "Time for epoch 31 is 0.2416 sec\n",
      "\tGenerator loss: 0.6649  Discriminator loss: 0.6838\n",
      "Time for epoch 32 is 0.2406 sec\n",
      "\tGenerator loss: 0.6462  Discriminator loss: 0.6961\n",
      "Time for epoch 33 is 0.2563 sec\n",
      "\tGenerator loss: 0.6246  Discriminator loss: 0.7106\n",
      "Time for epoch 34 is 0.2577 sec\n",
      "\tGenerator loss: 0.6075  Discriminator loss: 0.7229\n",
      "Time for epoch 35 is 0.2686 sec\n",
      "\tGenerator loss: 0.6041  Discriminator loss: 0.7269\n",
      "Time for epoch 36 is 0.2813 sec\n",
      "\tGenerator loss: 0.6081  Discriminator loss: 0.7256\n",
      "Time for epoch 37 is 0.2757 sec\n",
      "\tGenerator loss: 0.6197  Discriminator loss: 0.7190\n",
      "Time for epoch 38 is 0.2429 sec\n",
      "\tGenerator loss: 0.6371  Discriminator loss: 0.7077\n",
      "Time for epoch 39 is 0.2688 sec\n",
      "\tGenerator loss: 0.6577  Discriminator loss: 0.6941\n",
      "Time for epoch 40 is 0.2512 sec\n",
      "\tGenerator loss: 0.6738  Discriminator loss: 0.6854\n",
      "Time for epoch 41 is 0.2396 sec\n",
      "\tGenerator loss: 0.6782  Discriminator loss: 0.6851\n",
      "Time for epoch 42 is 0.2477 sec\n",
      "\tGenerator loss: 0.6721  Discriminator loss: 0.6912\n",
      "Time for epoch 43 is 0.2451 sec\n",
      "\tGenerator loss: 0.6538  Discriminator loss: 0.7040\n",
      "Time for epoch 44 is 0.2446 sec\n",
      "\tGenerator loss: 0.6292  Discriminator loss: 0.7215\n",
      "Time for epoch 45 is 0.2524 sec\n",
      "\tGenerator loss: 0.6198  Discriminator loss: 0.7313\n",
      "Time for epoch 46 is 0.2757 sec\n",
      "\tGenerator loss: 0.6226  Discriminator loss: 0.7322\n",
      "Time for epoch 47 is 0.2510 sec\n",
      "\tGenerator loss: 0.6327  Discriminator loss: 0.7242\n",
      "Time for epoch 48 is 0.2481 sec\n",
      "\tGenerator loss: 0.6452  Discriminator loss: 0.7112\n",
      "Time for epoch 49 is 0.2494 sec\n",
      "\tGenerator loss: 0.6544  Discriminator loss: 0.6983\n",
      "Time for epoch 50 is 0.2449 sec\n",
      "\tGenerator loss: 0.6661  Discriminator loss: 0.6860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tensorflow.python.keras.engine.functional.Functional at 0x7f6ee0259eb0>,\n",
       " <tensorflow.python.keras.engine.functional.Functional at 0x7f6ee0260520>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gan(diab_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d0a85d58",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 1 is 0.3696 sec\n",
      "\tGenerator loss: 0.8320  Discriminator loss: 0.6852\n",
      "Time for epoch 2 is 0.3418 sec\n",
      "\tGenerator loss: 0.8862  Discriminator loss: 0.6476\n",
      "Time for epoch 3 is 0.3247 sec\n",
      "\tGenerator loss: 0.9215  Discriminator loss: 0.6191\n",
      "Time for epoch 4 is 0.3372 sec\n",
      "\tGenerator loss: 0.9610  Discriminator loss: 0.5928\n",
      "Time for epoch 5 is 0.3485 sec\n",
      "\tGenerator loss: 0.9953  Discriminator loss: 0.5686\n",
      "Time for epoch 6 is 0.3699 sec\n",
      "\tGenerator loss: 1.0273  Discriminator loss: 0.5457\n",
      "Time for epoch 7 is 0.3765 sec\n",
      "\tGenerator loss: 1.0469  Discriminator loss: 0.5260\n",
      "Time for epoch 8 is 0.3551 sec\n",
      "\tGenerator loss: 1.0576  Discriminator loss: 0.5081\n",
      "Time for epoch 9 is 0.3581 sec\n",
      "\tGenerator loss: 1.0622  Discriminator loss: 0.4918\n",
      "Time for epoch 10 is 0.3264 sec\n",
      "\tGenerator loss: 1.0460  Discriminator loss: 0.4836\n",
      "Time for epoch 11 is 0.3501 sec\n",
      "\tGenerator loss: 1.0316  Discriminator loss: 0.4788\n",
      "Time for epoch 12 is 0.3251 sec\n",
      "\tGenerator loss: 1.0021  Discriminator loss: 0.4799\n",
      "Time for epoch 13 is 0.3404 sec\n",
      "\tGenerator loss: 0.9545  Discriminator loss: 0.4884\n",
      "Time for epoch 14 is 0.3311 sec\n",
      "\tGenerator loss: 0.9011  Discriminator loss: 0.5021\n",
      "Time for epoch 15 is 0.3242 sec\n",
      "\tGenerator loss: 0.8587  Discriminator loss: 0.5125\n",
      "Time for epoch 16 is 0.3426 sec\n",
      "\tGenerator loss: 0.8255  Discriminator loss: 0.5177\n",
      "Time for epoch 17 is 0.3318 sec\n",
      "\tGenerator loss: 0.8061  Discriminator loss: 0.5159\n",
      "Time for epoch 18 is 0.3380 sec\n",
      "\tGenerator loss: 0.8046  Discriminator loss: 0.5069\n",
      "Time for epoch 19 is 0.3426 sec\n",
      "\tGenerator loss: 0.8018  Discriminator loss: 0.5010\n",
      "Time for epoch 20 is 0.3312 sec\n",
      "\tGenerator loss: 0.7927  Discriminator loss: 0.5017\n",
      "Time for epoch 21 is 0.3408 sec\n",
      "\tGenerator loss: 0.7626  Discriminator loss: 0.5144\n",
      "Time for epoch 22 is 0.3329 sec\n",
      "\tGenerator loss: 0.7474  Discriminator loss: 0.5232\n",
      "Time for epoch 23 is 0.3427 sec\n",
      "\tGenerator loss: 0.7325  Discriminator loss: 0.5333\n",
      "Time for epoch 24 is 0.3258 sec\n",
      "\tGenerator loss: 0.7439  Discriminator loss: 0.5333\n",
      "Time for epoch 25 is 0.3249 sec\n",
      "\tGenerator loss: 0.7806  Discriminator loss: 0.5199\n",
      "Time for epoch 26 is 0.3403 sec\n",
      "\tGenerator loss: 0.8297  Discriminator loss: 0.4963\n",
      "Time for epoch 27 is 0.3193 sec\n",
      "\tGenerator loss: 0.8805  Discriminator loss: 0.4657\n",
      "Time for epoch 28 is 0.3292 sec\n",
      "\tGenerator loss: 0.9148  Discriminator loss: 0.4409\n",
      "Time for epoch 29 is 0.3239 sec\n",
      "\tGenerator loss: 0.9507  Discriminator loss: 0.4167\n",
      "Time for epoch 30 is 0.3338 sec\n",
      "\tGenerator loss: 0.9650  Discriminator loss: 0.4109\n",
      "Time for epoch 31 is 0.3246 sec\n",
      "\tGenerator loss: 0.8692  Discriminator loss: 0.4583\n",
      "Time for epoch 32 is 0.3201 sec\n",
      "\tGenerator loss: 0.7119  Discriminator loss: 0.5626\n",
      "Time for epoch 33 is 0.3274 sec\n",
      "\tGenerator loss: 0.7002  Discriminator loss: 0.6283\n",
      "Time for epoch 34 is 0.3148 sec\n",
      "\tGenerator loss: 0.7552  Discriminator loss: 0.6455\n",
      "Time for epoch 35 is 0.3259 sec\n",
      "\tGenerator loss: 0.8305  Discriminator loss: 0.6314\n",
      "Time for epoch 36 is 0.3268 sec\n",
      "\tGenerator loss: 0.8100  Discriminator loss: 0.6342\n",
      "Time for epoch 37 is 0.3235 sec\n",
      "\tGenerator loss: 0.8331  Discriminator loss: 0.6244\n",
      "Time for epoch 38 is 0.3268 sec\n",
      "\tGenerator loss: 0.9692  Discriminator loss: 0.5711\n",
      "Time for epoch 39 is 0.3261 sec\n",
      "\tGenerator loss: 1.0399  Discriminator loss: 0.5278\n",
      "Time for epoch 40 is 0.3452 sec\n",
      "\tGenerator loss: 1.0521  Discriminator loss: 0.5055\n",
      "Time for epoch 41 is 0.3274 sec\n",
      "\tGenerator loss: 1.0557  Discriminator loss: 0.4909\n",
      "Time for epoch 42 is 0.3446 sec\n",
      "\tGenerator loss: 1.0641  Discriminator loss: 0.4790\n",
      "Time for epoch 43 is 0.3230 sec\n",
      "\tGenerator loss: 1.1058  Discriminator loss: 0.4540\n",
      "Time for epoch 44 is 0.3189 sec\n",
      "\tGenerator loss: 1.1039  Discriminator loss: 0.4315\n",
      "Time for epoch 45 is 0.3589 sec\n",
      "\tGenerator loss: 1.1319  Discriminator loss: 0.4000\n",
      "Time for epoch 46 is 0.3220 sec\n",
      "\tGenerator loss: 1.1483  Discriminator loss: 0.3777\n",
      "Time for epoch 47 is 0.3713 sec\n",
      "\tGenerator loss: 1.1652  Discriminator loss: 0.3611\n",
      "Time for epoch 48 is 0.3210 sec\n",
      "\tGenerator loss: 1.2955  Discriminator loss: 0.3240\n",
      "Time for epoch 49 is 0.3295 sec\n",
      "\tGenerator loss: 1.4599  Discriminator loss: 0.2780\n",
      "Time for epoch 50 is 0.3469 sec\n",
      "\tGenerator loss: 1.4534  Discriminator loss: 0.2601\n",
      "Time for epoch 51 is 0.3799 sec\n",
      "\tGenerator loss: 1.3377  Discriminator loss: 0.2706\n",
      "Time for epoch 52 is 0.3645 sec\n",
      "\tGenerator loss: 1.2830  Discriminator loss: 0.2797\n",
      "Time for epoch 53 is 0.3290 sec\n",
      "\tGenerator loss: 1.3052  Discriminator loss: 0.2750\n",
      "Time for epoch 54 is 0.3427 sec\n",
      "\tGenerator loss: 1.1364  Discriminator loss: 0.3136\n",
      "Time for epoch 55 is 0.3268 sec\n",
      "\tGenerator loss: 0.9534  Discriminator loss: 0.3884\n",
      "Time for epoch 56 is 0.3294 sec\n",
      "\tGenerator loss: 0.9384  Discriminator loss: 0.4264\n",
      "Time for epoch 57 is 0.3444 sec\n",
      "\tGenerator loss: 0.9632  Discriminator loss: 0.4429\n",
      "Time for epoch 58 is 0.3252 sec\n",
      "\tGenerator loss: 0.9945  Discriminator loss: 0.4348\n",
      "Time for epoch 59 is 0.3442 sec\n",
      "\tGenerator loss: 0.8388  Discriminator loss: 0.4916\n",
      "Time for epoch 60 is 0.3253 sec\n",
      "\tGenerator loss: 0.8411  Discriminator loss: 0.5139\n",
      "Time for epoch 61 is 0.3486 sec\n",
      "\tGenerator loss: 0.9035  Discriminator loss: 0.5123\n",
      "Time for epoch 62 is 0.3461 sec\n",
      "\tGenerator loss: 0.8759  Discriminator loss: 0.5350\n",
      "Time for epoch 63 is 0.3414 sec\n",
      "\tGenerator loss: 1.0106  Discriminator loss: 0.5080\n",
      "Time for epoch 64 is 0.3308 sec\n",
      "\tGenerator loss: 1.1714  Discriminator loss: 0.4452\n",
      "Time for epoch 65 is 0.3344 sec\n",
      "\tGenerator loss: 1.0613  Discriminator loss: 0.4491\n",
      "Time for epoch 66 is 0.3314 sec\n",
      "\tGenerator loss: 1.0663  Discriminator loss: 0.4513\n",
      "Time for epoch 67 is 0.3420 sec\n",
      "\tGenerator loss: 1.0060  Discriminator loss: 0.4763\n",
      "Time for epoch 68 is 0.3545 sec\n",
      "\tGenerator loss: 0.9613  Discriminator loss: 0.5052\n",
      "Time for epoch 69 is 0.3292 sec\n",
      "\tGenerator loss: 0.9612  Discriminator loss: 0.5137\n",
      "Time for epoch 70 is 0.3445 sec\n",
      "\tGenerator loss: 1.1325  Discriminator loss: 0.4452\n",
      "Time for epoch 71 is 0.3615 sec\n",
      "\tGenerator loss: 1.2243  Discriminator loss: 0.3711\n",
      "Time for epoch 72 is 0.3365 sec\n",
      "\tGenerator loss: 1.1533  Discriminator loss: 0.3515\n",
      "Time for epoch 73 is 0.3439 sec\n",
      "\tGenerator loss: 1.0931  Discriminator loss: 0.3627\n",
      "Time for epoch 74 is 0.3285 sec\n",
      "\tGenerator loss: 0.9628  Discriminator loss: 0.4115\n",
      "Time for epoch 75 is 0.3330 sec\n",
      "\tGenerator loss: 0.9504  Discriminator loss: 0.4302\n",
      "Time for epoch 76 is 0.3262 sec\n",
      "\tGenerator loss: 1.0892  Discriminator loss: 0.3904\n",
      "Time for epoch 77 is 0.3436 sec\n",
      "\tGenerator loss: 1.1433  Discriminator loss: 0.3530\n",
      "Time for epoch 78 is 0.3396 sec\n",
      "\tGenerator loss: 1.3022  Discriminator loss: 0.3006\n",
      "Time for epoch 79 is 0.3259 sec\n",
      "\tGenerator loss: 1.3265  Discriminator loss: 0.2770\n",
      "Time for epoch 80 is 0.3615 sec\n",
      "\tGenerator loss: 0.9658  Discriminator loss: 0.3798\n",
      "Time for epoch 81 is 0.3355 sec\n",
      "\tGenerator loss: 1.3260  Discriminator loss: 0.3638\n",
      "Time for epoch 82 is 0.3444 sec\n",
      "\tGenerator loss: 1.5597  Discriminator loss: 0.2993\n",
      "Time for epoch 83 is 0.3409 sec\n",
      "\tGenerator loss: 1.5647  Discriminator loss: 0.2485\n",
      "Time for epoch 84 is 0.3268 sec\n",
      "\tGenerator loss: 1.6175  Discriminator loss: 0.2248\n",
      "Time for epoch 85 is 0.3469 sec\n",
      "\tGenerator loss: 1.5134  Discriminator loss: 0.2382\n",
      "Time for epoch 86 is 0.3495 sec\n",
      "\tGenerator loss: 1.2247  Discriminator loss: 0.3205\n",
      "Time for epoch 87 is 0.3408 sec\n",
      "\tGenerator loss: 1.5459  Discriminator loss: 0.3127\n",
      "Time for epoch 88 is 0.3205 sec\n",
      "\tGenerator loss: 1.4414  Discriminator loss: 0.3135\n",
      "Time for epoch 89 is 0.3400 sec\n",
      "\tGenerator loss: 1.3890  Discriminator loss: 0.3220\n",
      "Time for epoch 90 is 0.3331 sec\n",
      "\tGenerator loss: 1.2148  Discriminator loss: 0.3867\n",
      "Time for epoch 91 is 0.3305 sec\n",
      "\tGenerator loss: 1.1158  Discriminator loss: 0.4549\n",
      "Time for epoch 92 is 0.3396 sec\n",
      "\tGenerator loss: 0.9505  Discriminator loss: 0.5529\n",
      "Time for epoch 93 is 0.3291 sec\n",
      "\tGenerator loss: 0.9993  Discriminator loss: 0.5546\n",
      "Time for epoch 94 is 0.3587 sec\n",
      "\tGenerator loss: 1.0435  Discriminator loss: 0.5552\n",
      "Time for epoch 95 is 0.3199 sec\n",
      "\tGenerator loss: 1.3857  Discriminator loss: 0.4447\n",
      "Time for epoch 96 is 0.3320 sec\n",
      "\tGenerator loss: 1.3640  Discriminator loss: 0.3810\n",
      "Time for epoch 97 is 0.3299 sec\n",
      "\tGenerator loss: 1.2424  Discriminator loss: 0.4022\n",
      "Time for epoch 98 is 0.3239 sec\n",
      "\tGenerator loss: 1.2576  Discriminator loss: 0.4270\n",
      "Time for epoch 99 is 0.3307 sec\n",
      "\tGenerator loss: 1.4489  Discriminator loss: 0.4028\n",
      "Time for epoch 100 is 0.3131 sec\n",
      "\tGenerator loss: 1.7612  Discriminator loss: 0.3187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tensorflow.python.keras.engine.functional.Functional at 0x7f6ee020d850>,\n",
       " <tensorflow.python.keras.engine.functional.Functional at 0x7f6ee018e190>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gan(cred_x, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abb48ce",
   "metadata": {},
   "source": [
    "### Forgiving Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0b991c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "# @tf.function\n",
    "def train_step(samples, generator, discriminators, batch_size, genr_opt, disc_opt):\n",
    "    noise = tf.random.normal([batch_size, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_samples = generator(noise, training=True)\n",
    "\n",
    "        real_outputs = [disc(samples, training=True) for disc in discriminators]\n",
    "        fake_outputs = [disc(generated_samples, training=True) for disc in discriminators]\n",
    "\n",
    "        gen_loss = [generator_loss(fake_output) for fake_output in fake_outputs]\n",
    "        disc_loss = [discriminator_loss(real_output, fake_output) for real_output, fake_output in zip(real_outputs, fake_outputs)]\n",
    "\n",
    "    _ = [print(type(gl)) for gl in gen_loss]\n",
    "    gen_loss = tf.reduce_mean(gen_loss)\n",
    "    \n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    genr_opt.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    \n",
    "    for i in range(len(discriminators)):\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss[i], discriminators[i].trainable_variables)\n",
    "        disc_opt[i].apply_gradients(zip(gradients_of_discriminator, discriminators[i].trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cdbf323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(train_data, disc_num=1, epochs=50, noise_dim=5, batch_size=64, buffer_size=10000):\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_data).shuffle(buffer_size).batch(batch_size)\n",
    "    \n",
    "    discriminators = [discriminator_model(train_data.shape[1]) for _ in range(disc_num)]\n",
    "    generator = generator_model(noise_dim, train_data.shape[1])\n",
    "    \n",
    "    disc_opt = [Adam(1e-4) for _ in range(disc_num)]\n",
    "    genr_opt = Adam(1e-4)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        gen_loss = 0\n",
    "        disc_loss = 0\n",
    "        num_batchs = 0\n",
    "        \n",
    "        for sample_batch in train_dataset:\n",
    "            num_batchs += 1\n",
    "            curr_gen_loss, curr_disc_loss = train_step(sample_batch, generator, discriminators, batch_size, genr_opt, disc_opt)\n",
    "            gen_loss += curr_gen_loss\n",
    "            disc_loss += curr_disc_loss\n",
    "            \n",
    "        print (f'Time for epoch {epoch + 1} is {time.time()-start :.4f} sec')\n",
    "        print(f'\\tGenerator loss: {gen_loss/num_batchs:.4f}  Discriminator loss: {disc_loss/num_batchs:.4f}')\n",
    "        \n",
    "    return generator, discriminators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f718c40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: ['dense_112/kernel:0', 'dense_112/bias:0', 'dense_113/kernel:0', 'dense_113/bias:0', 'dense_114/kernel:0', 'dense_114/bias:0', 'dense_115/kernel:0', 'dense_115/bias:0'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-b5c6bdae650d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcred_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-107-c8e8a6d73929>\u001b[0m in \u001b[0;36mtrain_gan\u001b[0;34m(train_data, disc_num, epochs, noise_dim, batch_size, buffer_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msample_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mnum_batchs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mcurr_gen_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_disc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenr_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisc_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mgen_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcurr_gen_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mdisc_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcurr_disc_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-bd4b57e30460>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(samples, generator, discriminators, batch_size, genr_opt, disc_opt)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mgradients_of_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mgenr_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients_of_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-env/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcross\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mreplica\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m     \"\"\"\n\u001b[0;32m--> 598\u001b[0;31m     \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_empty_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-env/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/utils.py\u001b[0m in \u001b[0;36mfilter_empty_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[0m\u001b[1;32m     79\u001b[0m                      ([v.name for _, v in grads_and_vars],))\n\u001b[1;32m     80\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mvars_with_empty_grads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: ['dense_112/kernel:0', 'dense_112/bias:0', 'dense_113/kernel:0', 'dense_113/bias:0', 'dense_114/kernel:0', 'dense_114/bias:0', 'dense_115/kernel:0', 'dense_115/bias:0']."
     ]
    }
   ],
   "source": [
    "train_gan(cred_x, epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-env)",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
